{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One Batch Complete >>>\n",
      "One Batch Complete >>>\n",
      "One Batch Complete >>>\n",
      "One Batch Complete >>>\n",
      "One Batch Complete >>>\n",
      "\n",
      "There are total 738 sentences replaced by thier original data.\n"
     ]
    }
   ],
   "source": [
    "# Chinese translate to English\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_from_zh = pd.read_csv(\"NTCIR-13_MedWeb_en_from_zh_amazon_training.csv\")\n",
    "\n",
    "# remove punctuation inside\n",
    "import re\n",
    "def  clean_text(df, text_field):\n",
    "    df[text_field] = df[text_field].str.lower()\n",
    "    df[text_field] = df[text_field].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem))  \n",
    "    return df\n",
    "\n",
    "# update\n",
    "clean_train_from_zh = clean_text(train_from_zh,\"Tweet\")\n",
    "\n",
    "# lemmatization\n",
    "import spacy\n",
    "from vectorizers import SpacyLemmatizer\n",
    "\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "lemmatizer = SpacyLemmatizer(nlp, join_str=' ', n_threads=1)\n",
    "lemmas_train_from_zh = lemmatizer(list(clean_train_from_zh.Tweet)) \n",
    "\n",
    "# convert to list\n",
    "tweet_train_from_zh = list(lemmas_train_from_zh) \n",
    "\n",
    "# add stopwords \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop = list(stopwords.words('english'))\n",
    "stop.extend('haha ugghh ugh uh um oh ok okay boo damn god yu yike yen yay mikos mitsuru shirasu lol sigh 10 100 10000 1004 102 104 320000 968' .split())\n",
    "\n",
    "# using BOWs\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words=set(stop))\n",
    "X = vectorizer.fit_transform(tweet_train_from_zh).toarray()\n",
    "\n",
    "# assign labels\n",
    "categories = ['Influenza','Diarrhea','Hayfever','Cough','Headache','Fever','Runnynose','Cold']\n",
    "y = train_from_zh[categories].replace({'n':0, 'p':+1})\n",
    "y = y.values\n",
    "\n",
    "# generate new file\n",
    "import csv\n",
    "\n",
    "src_file_name1 = \"NTCIR-13_MedWeb_en_training.csv\"\n",
    "tran_file_name2 = \"NTCIR-13_MedWeb_en_from_zh_amazon_training.csv\"  # 源文件路径\n",
    "gen_file_name = \"error0.05_update_en_from_zh_training.csv\"  # 目标文件路径\n",
    "\n",
    "f1 = open(src_file_name1, 'r', encoding='UTF-8') \n",
    "f2 = open(tran_file_name2, 'r', encoding='UTF-8')\n",
    "f = open(gen_file_name, 'w', newline='') \n",
    "\n",
    "rows = csv.reader(f1)\n",
    "rows_tran = csv.reader(f2)\n",
    "write = csv.writer(f)\n",
    "write.writerow([\"ID\",\"Tweet\",\"Influenza\",\"Diarrhea\",\"Hayfever\",\"Cough\",\"Headache\",\"Fever\",\"Runnynose\",\"Cold\"])\n",
    "\n",
    "# count number of sentences replaced\n",
    "change_num = 0\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Using pipeline for applying logistic regression and one vs rest classifier\n",
    "LogReg_pipeline = Pipeline([\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(C=10, solver='lbfgs', penalty='l2', max_iter=3000, class_weight='balanced'), n_jobs=-1)),\n",
    "    ])\n",
    "\n",
    "mskf = MultilabelStratifiedKFold(n_splits=5, random_state=0)\n",
    "  \n",
    "for train_index, test_index in mskf.split(X, y):\n",
    "        \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    LogReg_pipeline.fit(X_train, y_train)\n",
    "    prediction = LogReg_pipeline.predict_proba(X_test)\n",
    "    print(\"One Batch Complete >>>\")\n",
    "    \n",
    "    row = y_test.shape[0]\n",
    "    col = y_test.shape[1]\n",
    "    \n",
    "    for r in range (0,row):\n",
    "        \n",
    "        # refresh\n",
    "        total_error = 0\n",
    "        for c in range (0,col):\n",
    "            error = y_test[r,c]-prediction[r,c]\n",
    "            abs_error = abs(error)\n",
    "            total_error = total_error+abs_error\n",
    "        # calculate average \n",
    "        ava_error = total_error/col\n",
    "        \n",
    "        # replace or keep sentence\n",
    "        if ava_error>=0.05:\n",
    "            change_num = change_num+1\n",
    "            for index, item in enumerate(rows):\n",
    "                if index == test_index[r]+1:\n",
    "                    write.writerow(item)\n",
    "                    f1.seek(0)\n",
    "                    break\n",
    "        else:\n",
    "            for index, item in enumerate(rows_tran):\n",
    "                if index == test_index[r]+1:\n",
    "                    write.writerow(item)\n",
    "                    f2.seek(0)\n",
    "                    break\n",
    "                \n",
    "\n",
    "f.close()\n",
    "print(\"\\nThere are total {} sentences replaced by thier original data.\".format(change_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One Batch Complete >>>\n",
      "One Batch Complete >>>\n",
      "One Batch Complete >>>\n",
      "One Batch Complete >>>\n",
      "One Batch Complete >>>\n",
      "\n",
      "There are total 677 sentences replaced by thier original data.\n"
     ]
    }
   ],
   "source": [
    "# Japanese translate to English\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_from_ja = pd.read_csv(\"NTCIR-13_MedWeb_en_from_ja_amazon_training.csv\")\n",
    "\n",
    "# remove punctuation inside\n",
    "import re\n",
    "def  clean_text(df, text_field):\n",
    "    df[text_field] = df[text_field].str.lower()\n",
    "    df[text_field] = df[text_field].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem))  \n",
    "    return df\n",
    "\n",
    "# update\n",
    "clean_train_from_ja = clean_text(train_from_ja,\"Tweet\")\n",
    "\n",
    "# lemmatization\n",
    "import spacy\n",
    "from vectorizers import SpacyLemmatizer\n",
    "\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "lemmatizer = SpacyLemmatizer(nlp, join_str=' ', n_threads=1)\n",
    "lemmas_train_from_ja = lemmatizer(list(clean_train_from_ja.Tweet)) \n",
    "\n",
    "# convert to list\n",
    "tweet_train_from_ja = list(lemmas_train_from_ja) \n",
    "\n",
    "# add stopwords \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop = list(stopwords.words('english'))\n",
    "stop.extend('haha ugghh ugh uh um oh ok okay boo damn god yu yike yen yay mikos mitsuru shirasu lol sigh 10 100 10000 1004 102 104 320000 968' .split())\n",
    "\n",
    "# using BOWs\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words=set(stop))\n",
    "X = vectorizer.fit_transform(tweet_train_from_ja).toarray()\n",
    "\n",
    "# assign labels\n",
    "categories = ['Influenza','Diarrhea','Hayfever','Cough','Headache','Fever','Runnynose','Cold']\n",
    "y = train_from_ja[categories].replace({'n':0, 'p':+1})\n",
    "y = y.values\n",
    "\n",
    "# generate new file\n",
    "\n",
    "import csv\n",
    "\n",
    "src_file_name1 = \"NTCIR-13_MedWeb_en_training.csv\"\n",
    "tran_file_name2 = \"NTCIR-13_MedWeb_en_from_ja_amazon_training.csv\"  # 源文件路径\n",
    "gen_file_name = \"error0.05_update_en_from_ja_training.csv\"  # 目标文件路径\n",
    "\n",
    "\n",
    "\n",
    "f1 = open(src_file_name1, 'r', encoding='UTF-8') \n",
    "f2 = open(tran_file_name2, 'r', encoding='UTF-8')\n",
    "f = open(gen_file_name, 'w', newline='') \n",
    "\n",
    "rows = csv.reader(f1)\n",
    "rows_tran = csv.reader(f2)\n",
    "write = csv.writer(f)\n",
    "write.writerow([\"ID\",\"Tweet\",\"Influenza\",\"Diarrhea\",\"Hayfever\",\"Cough\",\"Headache\",\"Fever\",\"Runnynose\",\"Cold\"])\n",
    "\n",
    "# count number of sentences replaced\n",
    "change_num = 0\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Using pipeline for applying logistic regression and one vs rest classifier\n",
    "LogReg_pipeline = Pipeline([\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(C=10, solver='lbfgs', penalty='l2', max_iter=3000, class_weight='balanced'), n_jobs=-1)),\n",
    "    ])\n",
    "\n",
    "mskf = MultilabelStratifiedKFold(n_splits=5, random_state=0)\n",
    "  \n",
    "for train_index, test_index in mskf.split(X, y):\n",
    "        \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    LogReg_pipeline.fit(X_train, y_train)\n",
    "    prediction = LogReg_pipeline.predict_proba(X_test)\n",
    "    print(\"One Batch Complete >>>\")\n",
    "    \n",
    "    row = y_test.shape[0]\n",
    "    col = y_test.shape[1]\n",
    "    \n",
    "    for r in range (0,row):\n",
    "        \n",
    "        # refresh\n",
    "        total_error = 0\n",
    "        for c in range (0,col):\n",
    "            error = y_test[r,c]-prediction[r,c]\n",
    "            abs_error = abs(error)\n",
    "            total_error = total_error+abs_error\n",
    "        # calculate average \n",
    "        ava_error = total_error/col\n",
    "        \n",
    "        # replace or keep sentence\n",
    "        if ava_error>=0.05:\n",
    "            change_num = change_num+1\n",
    "            for index, item in enumerate(rows):\n",
    "                if index == test_index[r]+1:\n",
    "                    write.writerow(item)\n",
    "                    f1.seek(0)\n",
    "                    break\n",
    "        else:\n",
    "            for index, item in enumerate(rows_tran):\n",
    "                if index == test_index[r]+1:\n",
    "                    write.writerow(item)\n",
    "                    f2.seek(0)\n",
    "                    break\n",
    "                \n",
    "\n",
    "f.close()\n",
    "print(\"\\nThere are total {} sentences replaced by thier original data.\".format(change_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One Batch Complete >>>\n",
      "One Batch Complete >>>\n",
      "One Batch Complete >>>\n",
      "One Batch Complete >>>\n",
      "One Batch Complete >>>\n",
      "\n",
      "There are total 655 sentences replaced by thier original data.\n"
     ]
    }
   ],
   "source": [
    "# English translate to Japanese\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_from_en = pd.read_csv(\"NTCIR-13_MedWeb_ja_from_en_amazon_training.csv\")\n",
    "\n",
    "# remove punctuations inside\n",
    "import re\n",
    "from zhon.hanzi import punctuation\n",
    "\n",
    "def clean_text(df, text_field):\n",
    "    \n",
    "    df[text_field] = df[text_field].apply(lambda elem:re.sub(r'[{}]+'.format(punctuation),'',elem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# update\n",
    "\n",
    "train_from_en = clean_text(train_from_en,\"Tweet\")\n",
    "tweet_train_from_en = list(train_from_en.Tweet)\n",
    "\n",
    "# Add Japanese tokenizer\n",
    "import nagisa\n",
    "\n",
    "def tokenize_jp(doc):\n",
    "    doc = nagisa.tagging(doc)\n",
    "    return doc.words\n",
    "\n",
    "# BOW\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "stop_words = ['!','0','1','2','3','4','6','8','9','?','、','。','〜','・','(',')',',','-','.','...','/']\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize_jp, stop_words=stop_words)\n",
    "X = vectorizer.fit_transform(tweet_train_from_en).toarray()\n",
    "\n",
    "# assigned labels\n",
    "categories = ['Influenza','Diarrhea','Hayfever','Cough','Headache','Fever','Runnynose','Cold']\n",
    "y = train_from_en[categories].replace({'n':0, 'p':+1})\n",
    "y = y.values\n",
    "\n",
    "# count number of sentences replaced\n",
    "change_num = 0\n",
    "\n",
    "# generate new file\n",
    "\n",
    "import csv\n",
    "\n",
    "src_file_name1 = \"NTCIR-13_MedWeb_ja_training.csv\"\n",
    "tran_file_name2 = \"NTCIR-13_MedWeb_ja_from_en_amazon_training.csv\"  # 源文件路径\n",
    "gen_file_name = \"error0.05_update_ja_from_en_training.csv\"  # 目标文件路径\n",
    "\n",
    "f1 = open(src_file_name1, 'r', encoding='utf-8') \n",
    "f2 = open(tran_file_name2, 'r', encoding='utf-8')\n",
    "f = open(gen_file_name, 'w', newline='') \n",
    "\n",
    "rows = csv.reader(f1)\n",
    "rows_tran = csv.reader(f2)\n",
    "write = csv.writer(f)\n",
    "write.writerow([\"ID\",\"Tweet\",\"Influenza\",\"Diarrhea\",\"Hayfever\",\"Cough\",\"Headache\",\"Fever\",\"Runnynose\",\"Cold\"])\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Using pipeline for applying logistic regression and one vs rest classifier\n",
    "LogReg_pipeline = Pipeline([\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(C=10, solver='lbfgs', penalty='l2', max_iter=3000, class_weight='balanced'), n_jobs=-1)),\n",
    "    ])\n",
    "\n",
    "mskf = MultilabelStratifiedKFold(n_splits=5, random_state=0)\n",
    "  \n",
    "for train_index, test_index in mskf.split(X, y):\n",
    "        \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    LogReg_pipeline.fit(X_train, y_train)\n",
    "    prediction = LogReg_pipeline.predict_proba(X_test)\n",
    "    print(\"One Batch Complete >>>\")\n",
    "    \n",
    "    row = y_test.shape[0]\n",
    "    col = y_test.shape[1]\n",
    "    \n",
    "    for r in range (0,row):\n",
    "        \n",
    "        # refresh\n",
    "        total_error = 0\n",
    "        for c in range (0,col):\n",
    "            error = y_test[r,c]-prediction[r,c]\n",
    "            abs_error = abs(error)\n",
    "            total_error = total_error+abs_error\n",
    "        # calculate average \n",
    "        ava_error = total_error/col\n",
    "        \n",
    "        # replace or keep sentence\n",
    "        if ava_error>=0.05:\n",
    "            change_num = change_num+1\n",
    "            for index, item in enumerate(rows):\n",
    "                if index == test_index[r]+1:\n",
    "                    write.writerow(item)\n",
    "                    f1.seek(0)\n",
    "                    break\n",
    "        else:\n",
    "            for index, item in enumerate(rows_tran):\n",
    "                if index == test_index[r]+1:\n",
    "                    write.writerow(item)\n",
    "                    f2.seek(0)\n",
    "                    break\n",
    "                \n",
    "\n",
    "f.close()\n",
    "print(\"\\nThere are total {} sentences replaced by thier original data.\".format(change_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One Batch Complete >>>\n",
      "One Batch Complete >>>\n",
      "One Batch Complete >>>\n",
      "One Batch Complete >>>\n",
      "One Batch Complete >>>\n",
      "\n",
      "There are total 696 sentences replaced by thier original data.\n"
     ]
    }
   ],
   "source": [
    "# Chinese translate to Japanese\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_from_zh = pd.read_csv(\"NTCIR-13_MedWeb_ja_from_zh_amazon_training.csv\")\n",
    "\n",
    "# remove punctuations inside\n",
    "import re\n",
    "from zhon.hanzi import punctuation\n",
    "\n",
    "def clean_text(df, text_field):\n",
    "    \n",
    "    df[text_field] = df[text_field].apply(lambda elem:re.sub(r'[{}]+'.format(punctuation),'',elem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# update\n",
    "\n",
    "train_from_zh = clean_text(train_from_zh,\"Tweet\")\n",
    "tweet_train_from_zh = list(train_from_zh.Tweet)\n",
    "\n",
    "# Add Japanese tokenizer\n",
    "import nagisa\n",
    "\n",
    "def tokenize_jp(doc):\n",
    "    doc = nagisa.tagging(doc)\n",
    "    return doc.words\n",
    "\n",
    "# BOW\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "stop_words = ['!','0','1','2','3','4','6','8','9','?','、','。','〜','・','(',')',',','-','.','...','/']\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize_jp, stop_words=stop_words)\n",
    "X = vectorizer.fit_transform(tweet_train_from_zh).toarray()\n",
    "\n",
    "# assigned labels\n",
    "categories = ['Influenza','Diarrhea','Hayfever','Cough','Headache','Fever','Runnynose','Cold']\n",
    "y = train_from_zh[categories].replace({'n':0, 'p':+1})\n",
    "y = y.values\n",
    "\n",
    "# count number of sentences replaced\n",
    "change_num = 0\n",
    "\n",
    "# generate new file\n",
    "\n",
    "import csv\n",
    "\n",
    "src_file_name1 = \"NTCIR-13_MedWeb_ja_training.csv\"\n",
    "tran_file_name2 = \"NTCIR-13_MedWeb_ja_from_zh_amazon_training.csv\"  # 源文件路径\n",
    "gen_file_name = \"error0.05_update_ja_from_zh_training.csv\"  # 目标文件路径\n",
    "\n",
    "\n",
    "\n",
    "f1 = open(src_file_name1, 'r', encoding='utf-8') \n",
    "f2 = open(tran_file_name2, 'r', encoding='utf-8')\n",
    "f = open(gen_file_name, 'w', newline='') \n",
    "\n",
    "rows = csv.reader(f1)\n",
    "rows_tran = csv.reader(f2)\n",
    "write = csv.writer(f)\n",
    "write.writerow([\"ID\",\"Tweet\",\"Influenza\",\"Diarrhea\",\"Hayfever\",\"Cough\",\"Headache\",\"Fever\",\"Runnynose\",\"Cold\"])\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Using pipeline for applying logistic regression and one vs rest classifier\n",
    "LogReg_pipeline = Pipeline([\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(C=10, solver='lbfgs', penalty='l2', max_iter=3000, class_weight='balanced'), n_jobs=-1)),\n",
    "    ])\n",
    "\n",
    "mskf = MultilabelStratifiedKFold(n_splits=5, random_state=0)\n",
    "  \n",
    "for train_index, test_index in mskf.split(X, y):\n",
    "        \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    LogReg_pipeline.fit(X_train, y_train)\n",
    "    prediction = LogReg_pipeline.predict_proba(X_test)\n",
    "    print(\"One Batch Complete >>>\")\n",
    "    \n",
    "    row = y_test.shape[0]\n",
    "    col = y_test.shape[1]\n",
    "    \n",
    "    for r in range (0,row):\n",
    "        \n",
    "        # refresh\n",
    "        total_error = 0\n",
    "        for c in range (0,col):\n",
    "            error = y_test[r,c]-prediction[r,c]\n",
    "            abs_error = abs(error)\n",
    "            total_error = total_error+abs_error\n",
    "        # calculate average \n",
    "        ava_error = total_error/col\n",
    "        \n",
    "        # replace or keep sentence\n",
    "        if ava_error>=0.05:\n",
    "            change_num = change_num+1\n",
    "            for index, item in enumerate(rows):\n",
    "                if index == test_index[r]+1:\n",
    "                    write.writerow(item)\n",
    "                    f1.seek(0)\n",
    "                    break\n",
    "        else:\n",
    "            for index, item in enumerate(rows_tran):\n",
    "                if index == test_index[r]+1:\n",
    "                    write.writerow(item)\n",
    "                    f2.seek(0)\n",
    "                    break\n",
    "                \n",
    "\n",
    "f.close()\n",
    "print(\"\\nThere are total {} sentences replaced by thier original data.\".format(change_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/xz/jrlb55l167x3c2jq466bx5fm0000gn/T/jieba.cache\n",
      "Loading model from cache /var/folders/xz/jrlb55l167x3c2jq466bx5fm0000gn/T/jieba.cache\n",
      "Loading model cost 0.849 seconds.\n",
      "Loading model cost 0.849 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One Batch Complete >>>\n",
      "One Batch Complete >>>\n",
      "One Batch Complete >>>\n",
      "One Batch Complete >>>\n",
      "One Batch Complete >>>\n",
      "\n",
      "There are total 605 sentences replaced by thier original data.\n"
     ]
    }
   ],
   "source": [
    "# English translate to Chinese\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_from_en = pd.read_csv(\"NTCIR-13_MedWeb_zh_from_en_amazon_training.csv\")\n",
    "\n",
    "# remove punctuations inside\n",
    "\n",
    "import re\n",
    "from zhon.hanzi import punctuation\n",
    "\n",
    "def clean_text(df, text_field):\n",
    "    \n",
    "    df[text_field] = df[text_field].apply(lambda elem:re.sub(r'[{}]+'.format(punctuation),'',elem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# update\n",
    "\n",
    "train_from_en = clean_text(train_from_en,\"Tweet\")\n",
    "tweet_train_from_en = list(train_from_en.Tweet)\n",
    "\n",
    "# import tokenizer\n",
    "\n",
    "import jieba\n",
    "\n",
    "def tokenize_zh(text):\n",
    "    words = jieba.lcut(text)\n",
    "    return words\n",
    "\n",
    "# assign labels\n",
    "\n",
    "categories = ['Influenza','Diarrhea','Hayfever','Cough','Headache','Fever','Runnynose','Cold']\n",
    "y = train_from_en[categories].replace({'n':0, 'p':+1})\n",
    "y = y.values\n",
    "\n",
    "# BOW\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "stop_words = ['1','100','3','32','36','38','39','40',\n",
    "              'ok','了',' ',',','.','...']\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize_zh, stop_words=stop_words)\n",
    "X = vectorizer.fit_transform(tweet_train_from_en).toarray()\n",
    "\n",
    "# count number of sentences replaced\n",
    "change_num = 0\n",
    "\n",
    "# generate new file\n",
    "\n",
    "import csv\n",
    "\n",
    "src_file_name1 = \"NTCIR-13_MedWeb_zh_training.csv\"\n",
    "tran_file_name2 = \"NTCIR-13_MedWeb_zh_from_en_amazon_training.csv\"  # 源文件路径\n",
    "gen_file_name = \"error0.05_update_zh_from_en_training.csv\"  # 目标文件路径\n",
    "\n",
    "\n",
    "\n",
    "f1 = open(src_file_name1, 'r', encoding='utf-8') \n",
    "f2 = open(tran_file_name2, 'r', encoding='utf-8')\n",
    "f = open(gen_file_name, 'w', newline='') \n",
    "\n",
    "rows = csv.reader(f1)\n",
    "rows_tran = csv.reader(f2)\n",
    "write = csv.writer(f)\n",
    "write.writerow([\"ID\",\"Tweet\",\"Influenza\",\"Diarrhea\",\"Hayfever\",\"Cough\",\"Headache\",\"Fever\",\"Runnynose\",\"Cold\"])\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Using pipeline for applying logistic regression and one vs rest classifier\n",
    "LogReg_pipeline = Pipeline([\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(C=10, solver='lbfgs', penalty='l2', max_iter=3000, class_weight='balanced'), n_jobs=-1)),\n",
    "    ])\n",
    "\n",
    "mskf = MultilabelStratifiedKFold(n_splits=5, random_state=0)\n",
    "  \n",
    "for train_index, test_index in mskf.split(X, y):\n",
    "        \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    LogReg_pipeline.fit(X_train, y_train)\n",
    "    prediction = LogReg_pipeline.predict_proba(X_test)\n",
    "    print(\"One Batch Complete >>>\")\n",
    "    \n",
    "    row = y_test.shape[0]\n",
    "    col = y_test.shape[1]\n",
    "    \n",
    "    for r in range (0,row):\n",
    "        \n",
    "        # refresh\n",
    "        total_error = 0\n",
    "        for c in range (0,col):\n",
    "            error = y_test[r,c]-prediction[r,c]\n",
    "            abs_error = abs(error)\n",
    "            total_error = total_error+abs_error\n",
    "        # calculate average \n",
    "        ava_error = total_error/col\n",
    "        \n",
    "        # replace or keep sentence\n",
    "        if ava_error>=0.05:\n",
    "            change_num = change_num+1\n",
    "            for index, item in enumerate(rows):\n",
    "                if index == test_index[r]+1:\n",
    "                    write.writerow(item)\n",
    "                    f1.seek(0)\n",
    "                    break\n",
    "        else:\n",
    "            for index, item in enumerate(rows_tran):\n",
    "                if index == test_index[r]+1:\n",
    "                    write.writerow(item)\n",
    "                    f2.seek(0)\n",
    "                    break\n",
    "                \n",
    "\n",
    "f.close()\n",
    "print(\"\\nThere are total {} sentences replaced by thier original data.\".format(change_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One Batch Complete >>>\n",
      "One Batch Complete >>>\n",
      "One Batch Complete >>>\n",
      "One Batch Complete >>>\n",
      "One Batch Complete >>>\n",
      "\n",
      "There are total 658 sentences replaced by thier original data.\n"
     ]
    }
   ],
   "source": [
    "# Japanese translate to Chinese\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_from_ja = pd.read_csv(\"NTCIR-13_MedWeb_zh_from_ja_amazon_training.csv\")\n",
    "\n",
    "# remove punctuations inside\n",
    "\n",
    "import re\n",
    "from zhon.hanzi import punctuation\n",
    "\n",
    "def clean_text(df, text_field):\n",
    "    \n",
    "    df[text_field] = df[text_field].apply(lambda elem:re.sub(r'[{}]+'.format(punctuation),'',elem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# update\n",
    "\n",
    "train_from_ja = clean_text(train_from_ja,\"Tweet\")\n",
    "tweet_train_from_ja = list(train_from_ja.Tweet)\n",
    "\n",
    "# import tokenizer\n",
    "\n",
    "import jieba\n",
    "\n",
    "def tokenize_zh(text):\n",
    "    words = jieba.lcut(text)\n",
    "    return words\n",
    "\n",
    "# assign labels\n",
    "\n",
    "categories = ['Influenza','Diarrhea','Hayfever','Cough','Headache','Fever','Runnynose','Cold']\n",
    "y = train_from_ja[categories].replace({'n':0, 'p':+1})\n",
    "y = y.values\n",
    "\n",
    "# BOW\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "stop_words = ['1','100','3','32','36','38','39','40',\n",
    "              'ok','了',' ',',','.','...']\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize_zh, stop_words=stop_words)\n",
    "X = vectorizer.fit_transform(tweet_train_from_ja).toarray()\n",
    "\n",
    "# count number of sentences replaced\n",
    "change_num = 0\n",
    "\n",
    "# generate new file\n",
    "\n",
    "import csv\n",
    "\n",
    "src_file_name1 = \"NTCIR-13_MedWeb_zh_training.csv\"\n",
    "tran_file_name2 = \"NTCIR-13_MedWeb_zh_from_ja_amazon_training.csv\"  # 源文件路径\n",
    "gen_file_name = \"error0.05_update_zh_from_ja_training.csv\"  # 目标文件路径\n",
    "\n",
    "\n",
    "\n",
    "f1 = open(src_file_name1, 'r', encoding='utf-8') \n",
    "f2 = open(tran_file_name2, 'r', encoding='utf-8')\n",
    "f = open(gen_file_name, 'w', newline='') \n",
    "\n",
    "rows = csv.reader(f1)\n",
    "rows_tran = csv.reader(f2)\n",
    "write = csv.writer(f)\n",
    "write.writerow([\"ID\",\"Tweet\",\"Influenza\",\"Diarrhea\",\"Hayfever\",\"Cough\",\"Headache\",\"Fever\",\"Runnynose\",\"Cold\"])\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Using pipeline for applying logistic regression and one vs rest classifier\n",
    "LogReg_pipeline = Pipeline([\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(C=10, solver='lbfgs', penalty='l2', max_iter=3000, class_weight='balanced'), n_jobs=-1)),\n",
    "    ])\n",
    "\n",
    "mskf = MultilabelStratifiedKFold(n_splits=5, random_state=0)\n",
    "  \n",
    "for train_index, test_index in mskf.split(X, y):\n",
    "        \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    LogReg_pipeline.fit(X_train, y_train)\n",
    "    prediction = LogReg_pipeline.predict_proba(X_test)\n",
    "    print(\"One Batch Complete >>>\")\n",
    "    \n",
    "    row = y_test.shape[0]\n",
    "    col = y_test.shape[1]\n",
    "    \n",
    "    for r in range (0,row):\n",
    "        \n",
    "        # refresh\n",
    "        total_error = 0\n",
    "        for c in range (0,col):\n",
    "            error = y_test[r,c]-prediction[r,c]\n",
    "            abs_error = abs(error)\n",
    "            total_error = total_error+abs_error\n",
    "        # calculate average \n",
    "        ava_error = total_error/col\n",
    "        \n",
    "        # replace or keep sentence\n",
    "        if ava_error>=0.05:\n",
    "            change_num = change_num+1\n",
    "            for index, item in enumerate(rows):\n",
    "                if index == test_index[r]+1:\n",
    "                    write.writerow(item)\n",
    "                    f1.seek(0)\n",
    "                    break\n",
    "        else:\n",
    "            for index, item in enumerate(rows_tran):\n",
    "                if index == test_index[r]+1:\n",
    "                    write.writerow(item)\n",
    "                    f2.seek(0)\n",
    "                    break\n",
    "                \n",
    "\n",
    "f.close()\n",
    "print(\"\\nThere are total {} sentences replaced by thier original data.\".format(change_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
